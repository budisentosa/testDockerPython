{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation - FIXED VERSION - Part 3\n",
    "\n",
    "## Clustering & Business Insights\n",
    "\n",
    "This notebook continues from Part 2 with feature preparation, clustering, and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkmagenta;text-align: center; background-color: AliceBlue;padding: 20px;\">7. Feature Preparation for Clustering</h2><a id=\"7\"></a>\n",
    "\n",
    "### \ud83d\udd34 CRITICAL FIX: Proper Handling of Categorical Variables\n",
    "\n",
    "**The Original Error:**\n",
    "```python\n",
    "# WRONG - Treating categorical as numeric!\n",
    "df['CustGender'] = df['CustGender'].map({'M': 1, 'F': 0})\n",
    "df_scaled = StandardScaler().fit_transform(df)  # Scales the 0/1 values!\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "1. Gender is NOMINAL (no order), not ordinal\n",
    "2. M=1, F=0 implies M > F (wrong!)\n",
    "3. StandardScaler transforms 0/1 to different values, losing meaning\n",
    "\n",
    "**The Correct Approach:**\n",
    "```python\n",
    "# CORRECT - One-hot encoding for categorical\n",
    "# Separate handling for numerical and categorical\n",
    "categorical \u2192 One-Hot Encoding \u2192 Keep as is (already 0/1)\n",
    "numerical \u2192 StandardScaler \u2192 Normalize for equal weighting\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why Feature Scaling Matters:\n",
    "\n",
    "K-Means uses **Euclidean distance**, so feature scales matter!\n",
    "\n",
    "**Example:**\n",
    "- Customer A: Recency=5 days, MonetaryTotal=\u20b910,000\n",
    "- Customer B: Recency=10 days, MonetaryTotal=\u20b915,000\n",
    "\n",
    "**Without scaling:**\n",
    "```\n",
    "Distance = \u221a[(5-10)\u00b2 + (10000-15000)\u00b2] = \u221a[25 + 25,000,000] \u2248 5,000\n",
    "```\n",
    "Monetary dominates! Recency difference (5 days) is ignored.\n",
    "\n",
    "**With scaling:**\n",
    "```\n",
    "Both features normalized to similar ranges\n",
    "Both contribute equally to distance calculation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customer dataset from Part 2\n",
    "customer_df = pd.read_csv('customer_rfm_features.csv')\n",
    "\n",
    "print(\"\ud83d\udcca Customer Dataset Loaded\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Shape: {customer_df.shape}\")\n",
    "print(f\"\\nColumns: {list(customer_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for clustering\n",
    "print(\"\ud83c\udfaf Selecting Features for Clustering\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Numerical features - will be scaled\n",
    "numerical_features = [\n",
    "    'Recency',           # Days since last transaction\n",
    "    'Frequency',         # Number of transactions\n",
    "    'MonetaryTotal',     # Total spending\n",
    "    'MonetaryAvg',       # Average transaction amount\n",
    "    'AccountBalance',    # Current account balance\n",
    "    'Age'                # Customer age\n",
    "]\n",
    "\n",
    "# Categorical features - will be one-hot encoded\n",
    "categorical_features = [\n",
    "    'Gender'             # M/F\n",
    "]\n",
    "\n",
    "# Features to exclude from clustering\n",
    "exclude_features = [\n",
    "    'CustomerID',         # Identifier (not a feature)\n",
    "    'LastTransactionDate',# Already captured in Recency\n",
    "    'Location',           # Too many categories (800+ cities)\n",
    "    'MonetaryStd',        # Volatility - can add if desired\n",
    "    'AccountBalanceAvg'   # Redundant with AccountBalance\n",
    "]\n",
    "\n",
    "print(f\"\\nNumerical features ({len(numerical_features)}):\")\n",
    "for feat in numerical_features:\n",
    "    print(f\"  \u2713 {feat}\")\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}):\")\n",
    "for feat in categorical_features:\n",
    "    print(f\"  \u2713 {feat}\")\n",
    "\n",
    "print(f\"\\nExcluded features ({len(exclude_features)}):\")\n",
    "for feat in exclude_features:\n",
    "    print(f\"  \u2717 {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Handle Categorical Variables - ONE-HOT ENCODING\n",
    "\n",
    "**What is One-Hot Encoding?**\n",
    "\n",
    "Converts categorical variables into binary columns:\n",
    "\n",
    "```\n",
    "Before:              After:\n",
    "Gender               Gender_F    Gender_M\n",
    "------               --------    --------\n",
    "  M                     0           1\n",
    "  F                     1           0\n",
    "  M                     0           1\n",
    "```\n",
    "\n",
    "**Why?**\n",
    "- No artificial ordering (M \u2260 1 > F = 0)\n",
    "- Each category equally important\n",
    "- Works with any ML algorithm\n",
    "\n",
    "**Note:** We use `drop_first=True` to avoid multicollinearity\n",
    "- If Gender_F = 0, then Gender_M = 1 (redundant)\n",
    "- Keep only one column (e.g., Gender_M where 1=Male, 0=Female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "print(\"\ud83d\udd27 One-Hot Encoding Categorical Variables\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Before encoding\n",
    "print(\"\\nBefore encoding:\")\n",
    "print(customer_df[categorical_features].head())\n",
    "print(f\"\\nGender distribution:\")\n",
    "print(customer_df['Gender'].value_counts())\n",
    "\n",
    "# Apply one-hot encoding\n",
    "df_categorical = pd.get_dummies(\n",
    "    customer_df[categorical_features],\n",
    "    drop_first=True,  # Avoid multicollinearity\n",
    "    prefix='Gender'\n",
    ")\n",
    "\n",
    "# After encoding\n",
    "print(\"\\nAfter encoding:\")\n",
    "print(df_categorical.head())\n",
    "print(f\"\\nNew columns: {list(df_categorical.columns)}\")\n",
    "print(f\"\\nGender_M = 1 means Male\")\n",
    "print(f\"Gender_M = 0 means Female\")\n",
    "\n",
    "print(\"\\n\u2713 Categorical variables encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scale Numerical Variables - STANDARDIZATION\n",
    "\n",
    "**What is StandardScaler?**\n",
    "\n",
    "Transforms each feature to have:\n",
    "- Mean = 0\n",
    "- Standard Deviation = 1\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "scaled_value = (original_value - mean) / std_deviation\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Recency: [1, 5, 10, 30, 365] \u2192 [-0.5, -0.4, -0.3, 0.2, 2.8]\n",
    "All features now on comparable scales!\n",
    "```\n",
    "\n",
    "**Why StandardScaler (not MinMaxScaler)?**\n",
    "- Works better with outliers\n",
    "- Preserves information about extremes\n",
    "- Standard choice for K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "print(\"\ud83d\udccf Scaling Numerical Features (StandardScaler)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Before scaling\n",
    "print(\"\\nBefore scaling (sample):\")\n",
    "print(customer_df[numerical_features].head())\n",
    "print(\"\\nStatistics before scaling:\")\n",
    "print(customer_df[numerical_features].describe().T[['mean', 'std']])\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_numerical_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(customer_df[numerical_features]),\n",
    "    columns=numerical_features,\n",
    "    index=customer_df.index\n",
    ")\n",
    "\n",
    "# After scaling\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"After scaling (sample):\")\n",
    "print(df_numerical_scaled.head())\n",
    "print(\"\\nStatistics after scaling:\")\n",
    "desc_stats = df_numerical_scaled.describe().T[['mean', 'std']].round(6)\nprint(desc_stats)\n",
    "print(\"\\n\u2713 All features now have mean \u2248 0 and std \u2248 1\")\n",
    "\n",
    "print(\"\\n\ud83d\udcbe Saving scaler for future use...\")\n",
    "joblib.dump(scaler, 'models/standard_scaler.pkl')\n",
    "print(\"   Scaler saved to 'models/standard_scaler.pkl'\")\n",
    "print(\"   Use this to scale new customers in production!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Combine Features - FINAL DATASET FOR CLUSTERING\n",
    "\n",
    "**Final dataset combines:**\n",
    "- Scaled numerical features (mean=0, std=1)\n",
    "- One-hot encoded categorical features (0 or 1)\n",
    "\n",
    "**Note:** Categorical features NOT scaled because:\n",
    "- Already binary (0 or 1)\n",
    "- Scaling would distort their meaning\n",
    "- They represent presence/absence, not magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine scaled numerical and one-hot encoded categorical\n",
    "print(\"\ud83d\udd17 Combining Features\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_clustering = pd.concat([\n",
    "    df_numerical_scaled,  # Scaled numerical\n",
    "    df_categorical        # One-hot encoded categorical\n",
    "], axis=1)\n",
    "\n",
    "print(f\"\\nFinal dataset for clustering:\")\n",
    "print(f\"  Shape: {df_clustering.shape}\")\n",
    "print(f\"  Rows (customers): {df_clustering.shape[0]:,}\")\n",
    "print(f\"  Columns (features): {df_clustering.shape[1]}\")\n",
    "print(f\"\\nFeatures:\")\n",
    "for col in df_clustering.columns:\n",
    "    print(f\"  \u2713 {col}\")\n",
    "\n",
    "print(\"\\nSample data:\")\n",
    "df_clustering.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Sampling Strategy (Optional)\n",
    "\n",
    "**The Dilemma:**\n",
    "- Full dataset: 800K+ customers (computationally expensive)\n",
    "- Sample: Faster, but may miss patterns\n",
    "\n",
    "**Options:**\n",
    "\n",
    "1. **Random Sampling** (Current approach)\n",
    "   - Pros: Fast, simple\n",
    "   - Cons: May miss rare segments\n",
    "\n",
    "2. **Stratified Sampling**\n",
    "   - Pros: Preserves proportions\n",
    "   - Cons: Need stratification variable\n",
    "\n",
    "3. **Mini-Batch K-Means** (Best for large data)\n",
    "   - Pros: Handles full dataset efficiently\n",
    "   - Cons: Slightly less accurate\n",
    "\n",
    "**Our Choice:** Random sampling of 100K customers for speed\n",
    "- Represents ~12.5% of data\n",
    "- Still statistically significant\n",
    "- Faster iteration for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling (if configured)\n",
    "if CONFIG.get('USE_SAMPLING', True) and CONFIG.get('SAMPLE_SIZE'):\n",
    "    print(\"\ud83d\udcca Sampling Strategy\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Full dataset size: {len(df_clustering):,} customers\")\n",
    "    \n",
    "    sample_size = min(CONFIG['SAMPLE_SIZE'], len(df_clustering))\n",
    "    df_final = df_clustering.sample(\n",
    "        n=sample_size,\n",
    "        random_state=CONFIG['RANDOM_STATE']\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Sample size: {len(df_final):,} customers\")\n",
    "    print(f\"Sampling rate: {len(df_final)/len(df_clustering)*100:.1f}%\")\n",
    "    print(f\"\\n\u26a0\ufe0f  Note: Using sample for faster computation\")\n",
    "    print(f\"   For production, consider Mini-Batch K-Means on full dataset\")\n",
    "else:\n",
    "    print(\"Using full dataset (no sampling)\")\n",
    "    df_final = df_clustering.copy()\n",
    "\n",
    "print(f\"\\n\u2713 Final dataset ready: {len(df_final):,} customers \u00d7 {df_final.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save categorical columns list for later reference\n",
    "categorical_cols = list(df_categorical.columns)\n",
    "joblib.dump(categorical_cols, 'models/categorical_columns.pkl')\n",
    "print(\"\ud83d\udcbe Categorical column names saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \u2705 Feature Preparation Complete!\n",
    "\n",
    "**What we accomplished:**\n",
    "1. \u2713 Separated numerical and categorical features\n",
    "2. \u2713 One-hot encoded categorical variables (NO fake ordering!)\n",
    "3. \u2713 Scaled numerical variables using StandardScaler\n",
    "4. \u2713 Combined features properly\n",
    "5. \u2713 Applied sampling strategy\n",
    "6. \u2713 Saved preprocessing objects for production use\n",
    "\n",
    "**Critical Fixes Applied:**\n",
    "- \u274c Original: `Gender \u2192 {M:1, F:0} \u2192 StandardScaler()` (WRONG!)\n",
    "- \u2705 Fixed: `Gender \u2192 One-Hot \u2192 Keep binary` (CORRECT!)\n",
    "\n",
    "**Ready for clustering!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}