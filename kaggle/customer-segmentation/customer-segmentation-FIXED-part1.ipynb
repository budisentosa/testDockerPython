{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkmagenta; font-family:Cursive\"><center><b>Customer Segmentation - FIXED VERSION</center></h2>\n",
    "\n",
    "<img src=\"https://img2.storyblok.com/1120x292/filters:format(webp)/f/47007/2400x626/36e957bd2b/221005_customersegmentation_blog_teaser_v01.png\" alt=\"Customer Segmentation\" class=\"center\">\n",
    "\n",
    "---\n",
    "\n",
    "## üìã What's Fixed in This Version\n",
    "\n",
    "### Critical Fixes:\n",
    "1. ‚úÖ **Corrected RFM Calculation** - Recency now properly calculated as days since last transaction\n",
    "2. ‚úÖ **Proper Missing Data Analysis** - Analyze before dropping, document impact\n",
    "3. ‚úÖ **Correct Categorical Handling** - One-hot encoding instead of incorrect scaling\n",
    "4. ‚úÖ **Enhanced Cluster Interpretation** - Business insights and actionable recommendations\n",
    "5. ‚úÖ **Better Sampling Strategy** - Documented and justified approach\n",
    "6. ‚úÖ **Added Validation** - Model validation and stability checks\n",
    "\n",
    "### Additional Improvements:\n",
    "- Detailed step-by-step explanations\n",
    "- Code organized into reusable functions\n",
    "- Business context for each analysis\n",
    "- Statistical validation throughout\n",
    "- Production-ready model persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:MediumVioletRed; font-family:Cursive\"><b>About the Data üí°</h2>\n",
    "\n",
    "* This dataset consists of 1 Million+ transactions by over 800K customers for a bank in India.\n",
    "* The data contains information such as:\n",
    "  - Customer demographics (DOB, gender, location)\n",
    "  - Account information (balance)\n",
    "  - Transaction details (date, amount, ID)\n",
    "\n",
    "**Business Context:**\n",
    "Understanding customer segments helps the bank:\n",
    "- Personalize marketing campaigns\n",
    "- Identify high-value customers for retention\n",
    "- Detect at-risk customers for reactivation\n",
    "- Optimize resource allocation\n",
    "- Improve customer experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:MediumVioletRed; font-family:Cursive\"><b>Analysis Goals üéØ</h2>\n",
    "\n",
    "1. ‚úÖ Perform customer segmentation using RFM analysis and K-Means clustering\n",
    "2. ‚úÖ Identify distinct customer groups with clear business definitions\n",
    "3. ‚úÖ Provide actionable insights for each segment\n",
    "4. ‚úÖ Validate cluster quality and stability\n",
    "5. ‚úÖ Create production-ready segmentation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents üì≠**\n",
    "\n",
    "1. [Setup & Configuration](#1)\n",
    "2. [Data Collection](#2)\n",
    "3. [Data Quality Analysis](#3)\n",
    "4. [Data Cleaning](#4)\n",
    "5. [Feature Engineering - RFM](#5)\n",
    "6. [Exploratory Data Analysis](#6)\n",
    "7. [Feature Preparation for Clustering](#7)\n",
    "8. [Optimal Cluster Selection](#8)\n",
    "9. [K-Means Clustering](#9)\n",
    "10. [Cluster Interpretation & Business Insights](#10)\n",
    "11. [PCA Analysis](#11)\n",
    "12. [Model Validation](#12)\n",
    "13. [Model Persistence](#13)\n",
    "14. [Conclusions & Recommendations](#14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkmagenta;text-align: center; background-color: AliceBlue;padding: 20px;\">1. Setup & Configuration</h2><a id=\"1\"></a>\n",
    "\n",
    "### Why Configuration Matters:\n",
    "- Centralizes all parameters for easy tuning\n",
    "- Makes notebook reproducible\n",
    "- Follows software engineering best practices\n",
    "- Easy to convert to production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotly for interactive visualizations\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "except:\n",
    "    !pip install plotly\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "# Clustering and preprocessing\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Additional utilities\n",
    "try:\n",
    "    from kneed import KneeLocator\n",
    "except:\n",
    "    !pip install kneed\n",
    "    from kneed import KneeLocator\n",
    "\n",
    "try:\n",
    "    from yellowbrick.cluster import KElbowVisualizer\n",
    "except:\n",
    "    !pip install -U yellowbrick\n",
    "    from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# For model persistence\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Dictionary\n",
    "# Centralized configuration makes the notebook easy to modify and reproduce\n",
    "\n",
    "CONFIG = {\n",
    "    # Data paths\n",
    "    'DATA_PATH': '/kaggle/input/bank-customer-segmentation/bank_transactions.csv',\n",
    "    'MODEL_OUTPUT_DIR': 'models/',\n",
    "    \n",
    "    # Sampling (use None for full dataset with MiniBatch KMeans)\n",
    "    'SAMPLE_SIZE': 100000,  # Sample size for faster processing\n",
    "    'USE_SAMPLING': True,   # Set False to use full dataset\n",
    "    \n",
    "    # Random state for reproducibility\n",
    "    'RANDOM_STATE': 42,\n",
    "    \n",
    "    # Clustering parameters\n",
    "    'N_CLUSTERS_MIN': 2,\n",
    "    'N_CLUSTERS_MAX': 10,\n",
    "    'OPTIMAL_K': 5,  # Will be determined by analysis\n",
    "    \n",
    "    # PCA parameters\n",
    "    'PCA_VARIANCE_THRESHOLD': 0.90,  # Retain 90% variance\n",
    "    'PCA_N_COMPONENTS': 4,\n",
    "    \n",
    "    # Validation\n",
    "    'TEST_SIZE': 0.2,  # 20% for validation\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkmagenta;text-align: center; background-color: AliceBlue;padding: 20px;\">2. Data Collection</h2><a id=\"2\"></a>\n",
    "\n",
    "### What We're Doing:\n",
    "Loading the raw transaction data and performing initial inspection\n",
    "\n",
    "### Why It Matters:\n",
    "Understanding data structure is the foundation of good analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_raw = pd.read_csv(CONFIG['DATA_PATH'])\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df_raw.shape[0]:,} rows √ó {df_raw.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(\"Dataset Info:\")\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkmagenta;text-align: center; background-color: AliceBlue;padding: 20px;\">3. Data Quality Analysis</h2><a id=\"3\"></a>\n",
    "\n",
    "### What We're Doing:\n",
    "Comprehensive analysis of data quality issues BEFORE making any changes\n",
    "\n",
    "### Why It Matters:\n",
    "- Understanding the extent of data quality issues\n",
    "- Making informed decisions about cleaning strategies\n",
    "- Documenting what we're changing and why\n",
    "- Detecting potential data collection problems\n",
    "\n",
    "### Key Questions:\n",
    "1. How much missing data do we have?\n",
    "2. Are there duplicates?\n",
    "3. Are there data type issues?\n",
    "4. Are there suspicious values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive data quality report\n",
    "def create_data_quality_report(df):\n",
    "    \"\"\"\n",
    "    Creates a comprehensive data quality report\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with quality metrics for each column\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        report.append({\n",
    "            'Column': col,\n",
    "            'DType': df[col].dtype,\n",
    "            'Unique_Values': df[col].nunique(),\n",
    "            'Missing_Count': df[col].isnull().sum(),\n",
    "            'Missing_Pct': round(df[col].isnull().sum() / len(df) * 100, 2),\n",
    "            'Sample_Values': df[col].dropna().head(3).tolist()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(report)\n",
    "\n",
    "quality_report = create_data_quality_report(df_raw)\n",
    "print(\"üìä Data Quality Report:\")\n",
    "print(\"=\" * 80)\n",
    "quality_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing data\n",
    "print(\"üìâ Missing Data Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "missing_data = quality_report[quality_report['Missing_Count'] > 0]\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    print(f\"\\nColumns with missing data: {len(missing_data)}\")\n",
    "    print(missing_data[['Column', 'Missing_Count', 'Missing_Pct']])\n",
    "    \n",
    "    total_rows = len(df_raw)\n",
    "    rows_with_any_missing = df_raw.isnull().any(axis=1).sum()\n",
    "    print(f\"\\nRows with ANY missing value: {rows_with_any_missing:,} ({rows_with_any_missing/total_rows*100:.2f}%)\")\n",
    "    print(f\"\\n‚ö†Ô∏è  If we drop all rows with missing data, we lose {rows_with_any_missing:,} rows\")\n",
    "else:\n",
    "    print(\"‚úì No missing data found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(\"üîç Duplicate Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "duplicate_rows = df_raw.duplicated().sum()\n",
    "duplicate_transactions = df_raw.duplicated(subset=['TransactionID']).sum()\n",
    "duplicate_customers = df_raw['CustomerID'].duplicated().sum()\n",
    "\n",
    "print(f\"Duplicate rows (all columns): {duplicate_rows:,}\")\n",
    "print(f\"Duplicate TransactionIDs: {duplicate_transactions:,}\")\n",
    "print(f\"Note: Same customer with multiple transactions: {duplicate_customers:,} (This is EXPECTED)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical variables for data quality issues\n",
    "print(\"üìä Categorical Variable Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Gender distribution\n",
    "print(\"\\nGender Distribution:\")\n",
    "gender_counts = df_raw['CustGender'].value_counts()\n",
    "print(gender_counts)\n",
    "print(f\"\\nUnique values: {df_raw['CustGender'].unique()}\")\n",
    "\n",
    "# Check for unexpected values\n",
    "expected_genders = ['M', 'F']\n",
    "unexpected_genders = df_raw[~df_raw['CustGender'].isin(expected_genders)]\n",
    "if len(unexpected_genders) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Found {len(unexpected_genders):,} rows with unexpected gender values\")\n",
    "    print(f\"   This is {len(unexpected_genders)/len(df_raw)*100:.3f}% of data\")\n",
    "    print(f\"   Values: {unexpected_genders['CustGender'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Data Quality Summary\n",
    "\n",
    "Based on the analysis above, document key findings:\n",
    "\n",
    "**Missing Data:**\n",
    "- [To be filled based on output]\n",
    "\n",
    "**Duplicates:**\n",
    "- [To be filled based on output]\n",
    "\n",
    "**Data Quality Issues:**\n",
    "- Gender column may contain unexpected values (e.g., 'T')\n",
    "- Need to verify TransactionTime format\n",
    "- Need to check for negative or impossible ages\n",
    "\n",
    "**Next Steps:**\n",
    "Based on findings, we'll develop appropriate cleaning strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkmagenta;text-align: center; background-color: AliceBlue;padding: 20px;\">4. Data Cleaning</h2><a id=\"4\"></a>\n",
    "\n",
    "### Cleaning Strategy:\n",
    "Based on the quality analysis, we'll apply targeted cleaning:\n",
    "\n",
    "1. **Date Conversion** - Convert string dates to datetime objects\n",
    "2. **Age Calculation** - Calculate customer age at transaction time\n",
    "3. **Missing Data** - Strategic handling (drop only if minimal or impute if possible)\n",
    "4. **Gender Data** - Handle unexpected values appropriately\n",
    "5. **Outliers** - Analyze but preserve valid extreme values\n",
    "\n",
    "### Important Note:\n",
    "We create a copy (`df`) to preserve the raw data (`df_raw`) for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy\n",
    "df = df_raw.copy()\n",
    "print(f\"Starting with: {len(df):,} rows\")\n",
    "print(\"\\nCleaning steps:\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert date columns\n",
    "print(\"\\n1Ô∏è‚É£  Converting date columns...\")\n",
    "\n",
    "df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], errors='coerce')\n",
    "df['CustomerDOB'] = pd.to_datetime(df['CustomerDOB'], errors='coerce')\n",
    "\n",
    "# Check for any date parsing errors\n",
    "date_errors = df['TransactionDate'].isnull().sum() + df['CustomerDOB'].isnull().sum()\n",
    "print(f\"   Date parsing errors: {date_errors}\")\n",
    "print(f\"   Date range: {df['TransactionDate'].min()} to {df['TransactionDate'].max()}\")\n",
    "print(\"   ‚úì Dates converted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate customer age\n",
    "print(\"\\n2Ô∏è‚É£  Calculating customer age...\")\n",
    "\n",
    "# Calculate age in years at time of transaction\n",
    "# Using days/365.25 to account for leap years\n",
    "df['CustomerAge'] = (df['TransactionDate'] - df['CustomerDOB']).dt.days / 365.25\n",
    "\n",
    "# Analyze age distribution\n",
    "print(f\"   Age range: {df['CustomerAge'].min():.1f} to {df['CustomerAge'].max():.1f} years\")\n",
    "print(f\"   Mean age: {df['CustomerAge'].mean():.1f} years\")\n",
    "print(f\"   Median age: {df['CustomerAge'].median():.1f} years\")\n",
    "\n",
    "# Check for suspicious ages\n",
    "negative_age = (df['CustomerAge'] < 0).sum()\n",
    "very_young = (df['CustomerAge'] < 1).sum()\n",
    "very_old = (df['CustomerAge'] > 100).sum()\n",
    "\n",
    "if negative_age > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Negative ages: {negative_age:,} (data quality issue)\")\n",
    "if very_young > 0:\n",
    "    print(f\"   ‚ÑπÔ∏è  Age < 1 year: {very_young:,} (possibly child accounts)\")\n",
    "if very_old > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Age > 100 years: {very_old:,} (verify these)\")\n",
    "\n",
    "print(\"   ‚úì Age calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Handle TransactionTime column\n",
    "print(\"\\n3Ô∏è‚É£  Analyzing TransactionTime...\")\n",
    "\n",
    "if 'TransactionTime' in df.columns:\n",
    "    print(f\"   Sample values: {df['TransactionTime'].head(10).tolist()}\")\n",
    "    print(f\"   Unique values: {df['TransactionTime'].nunique():,}\")\n",
    "    print(f\"   Data type: {df['TransactionTime'].dtype}\")\n",
    "    \n",
    "    # Decision: Drop if unclear format, keep if useful\n",
    "    print(\"   ‚ö†Ô∏è  Format unclear - dropping column\")\n",
    "    print(\"   ‚ÑπÔ∏è  Note: Could be valuable for time-based analysis if format is understood\")\n",
    "    df.drop(columns=['TransactionTime'], inplace=True)\n",
    "    print(\"   ‚úì Column dropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Handle Gender data\n",
    "print(\"\\n4Ô∏è‚É£  Cleaning gender data...\")\n",
    "\n",
    "print(\"   Before cleaning:\")\n",
    "print(df['CustGender'].value_counts())\n",
    "\n",
    "# Filter to valid genders\n",
    "valid_genders = ['M', 'F']\n",
    "invalid_gender_count = len(df[~df['CustGender'].isin(valid_genders)])\n",
    "\n",
    "if invalid_gender_count > 0:\n",
    "    print(f\"\\n   Removing {invalid_gender_count:,} rows with invalid gender ({invalid_gender_count/len(df)*100:.3f}%)\")\n",
    "    df = df[df['CustGender'].isin(valid_genders)]\n",
    "    print(\"   ‚úì Invalid genders removed\")\n",
    "\n",
    "print(\"\\n   After cleaning:\")\n",
    "print(df['CustGender'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Handle missing data strategically\n",
    "print(\"\\n5Ô∏è‚É£  Handling missing data...\")\n",
    "\n",
    "print(f\"   Rows before: {len(df):,}\")\n",
    "\n",
    "# Critical columns that must have values\n",
    "critical_columns = ['CustomerID', 'TransactionID', 'TransactionDate', 'TransactionAmount (INR)']\n",
    "\n",
    "# Drop rows missing critical data\n",
    "df_before = len(df)\n",
    "df = df.dropna(subset=critical_columns)\n",
    "rows_dropped = df_before - len(df)\n",
    "\n",
    "if rows_dropped > 0:\n",
    "    print(f\"   Dropped {rows_dropped:,} rows missing critical data ({rows_dropped/df_before*100:.2f}%)\")\n",
    "\n",
    "# For non-critical columns, could impute or drop\n",
    "remaining_nulls = df.isnull().sum().sum()\n",
    "if remaining_nulls > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  {remaining_nulls:,} remaining null values in non-critical columns\")\n",
    "    df = df.dropna()  # Drop remaining\n",
    "    print(f\"   Dropped rows with remaining nulls\")\n",
    "\n",
    "print(f\"   Rows after: {len(df):,}\")\n",
    "print(f\"   Data retained: {len(df)/len(df_raw)*100:.2f}%\")\n",
    "print(\"   ‚úì Missing data handled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Remove duplicates\n",
    "print(\"\\n6Ô∏è‚É£  Removing duplicates...\")\n",
    "\n",
    "df_before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "duplicates_removed = df_before - len(df)\n",
    "\n",
    "if duplicates_removed > 0:\n",
    "    print(f\"   Removed {duplicates_removed:,} duplicate rows\")\n",
    "else:\n",
    "    print(\"   ‚úì No duplicates found\")\n",
    "\n",
    "print(f\"   Final row count: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleaning summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä CLEANING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Started with:     {len(df_raw):,} rows\")\n",
    "print(f\"Ended with:       {len(df):,} rows\")\n",
    "print(f\"Rows removed:     {len(df_raw) - len(df):,} ({(len(df_raw)-len(df))/len(df_raw)*100:.2f}%)\")\n",
    "print(f\"Data retained:    {len(df)/len(df_raw)*100:.2f}%\")\n",
    "print(f\"\\nUnique customers: {df['CustomerID'].nunique():,}\")\n",
    "print(f\"Unique transactions: {df['TransactionID'].nunique():,}\")\n",
    "print(f\"Date range: {df['TransactionDate'].min().date()} to {df['TransactionDate'].max().date()}\")\n",
    "print(f\"Days covered: {(df['TransactionDate'].max() - df['TransactionDate'].min()).days} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data quality after cleaning\n",
    "print(\"\\n‚úì Data Quality After Cleaning:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Data Cleaning Complete\n",
    "\n",
    "**What we accomplished:**\n",
    "1. ‚úì Converted dates to proper datetime format\n",
    "2. ‚úì Calculated customer age correctly\n",
    "3. ‚úì Removed unclear TransactionTime column\n",
    "4. ‚úì Cleaned gender data (removed invalid values)\n",
    "5. ‚úì Handled missing data strategically\n",
    "6. ‚úì Removed duplicates\n",
    "\n",
    "**Data is now ready for feature engineering!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
