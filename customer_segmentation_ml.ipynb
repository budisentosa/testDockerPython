{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banking Customer Segmentation Analysis\n",
    "## Machine Learning-Based Segmentation for High-Value Identification, Churn Prediction & Marketing\n",
    "\n",
    "**Objectives:**\n",
    "1. Identify high-value customers for priority service\n",
    "2. Predict churn risk (salary sweepers)\n",
    "3. Create actionable marketing segments\n",
    "\n",
    "**Approach:** Unsupervised K-Means clustering on 2M+ transaction records\n",
    "\n",
    "**Data Sources:**\n",
    "- 1M account records\n",
    "- 20K customer demographics (sample)\n",
    "- 157K core banking transactions\n",
    "- 856K merchant/mPOS transactions\n",
    "- 20K mobile banking transactions\n",
    "- 20K ATM/EDC transactions (sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def validate_data_load(df, name, min_rows=100):\n",
    "    \"\"\"Validate that data loaded correctly\"\"\"\n",
    "    if df is None or len(df) < min_rows:\n",
    "        print(f\"⚠️  WARNING: {name} has insufficient data ({len(df) if df is not None else 0} rows)\")\n",
    "        return False\n",
    "    print(f\"✓ {name}: {len(df):,} rows loaded\")\n",
    "    return True\n",
    "\n",
    "def report_join_coverage(df_left, df_right, key, join_type='left'):\n",
    "    \"\"\"Report join coverage statistics\"\"\"\n",
    "    before = len(df_left)\n",
    "    merged = pd.merge(df_left, df_right, on=key, how=join_type, indicator=True)\n",
    "    matched = (merged['_merge'] == 'both').sum()\n",
    "    print(f\"Join coverage on '{key}': {matched:,}/{before:,} ({matched/before*100:.1f}%)\")\n",
    "    return merged.drop('_merge', axis=1)\n",
    "\n",
    "def cap_outliers(series, percentile=99):\n",
    "    \"\"\"Cap outliers at specified percentile\"\"\"\n",
    "    cap_value = series.quantile(percentile / 100)\n",
    "    return series.clip(upper=cap_value)\n",
    "\n",
    "def memory_usage_summary(df, name):\n",
    "    \"\"\"Print memory usage summary\"\"\"\n",
    "    mem_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"{name} memory usage: {mem_mb:.2f} MB\")\n",
    "    return mem_mb\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading & Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Account Master Data (1M records - optimized loading)\n",
    "print(\"Loading account data...\")\n",
    "df_account = pd.read_csv(\n",
    "    'datasample/data_account_202601081013.csv',\n",
    "    usecols=['cif', 'acct_no', 'curr_bal', 'avg_bal', 'open_dt', 'stat_cd', 'prod_cd'],\n",
    "    dtype={'cif': str, 'acct_no': str, 'stat_cd': str, 'prod_cd': str},\n",
    "    parse_dates=['open_dt']\n",
    ")\n",
    "validate_data_load(df_account, \"Account Data\")\n",
    "memory_usage_summary(df_account, \"Account\")\n",
    "\n",
    "# Load Customer Demographics (20K - SAMPLE)\n",
    "print(\"\\nLoading customer demographics...\")\n",
    "df_nasabah = pd.read_csv(\n",
    "    'datasample/data_nasabah_202601081038.csv',\n",
    "    dtype={'cif': str}\n",
    ")\n",
    "validate_data_load(df_nasabah, \"Customer Demographics\")\n",
    "memory_usage_summary(df_nasabah, \"Demographics\")\n",
    "\n",
    "# Load Core Banking Transactions\n",
    "print(\"\\nLoading core banking transactions...\")\n",
    "df_core = pd.read_csv(\n",
    "    'datasample/transaksi_core_202601081036.csv',\n",
    "    dtype={'DTACCT': str, 'DTDATE': str}\n",
    ")\n",
    "validate_data_load(df_core, \"Core Banking Transactions\")\n",
    "memory_usage_summary(df_core, \"Core Transactions\")\n",
    "\n",
    "# Load Mobile Banking Transactions\n",
    "print(\"\\nLoading mobile banking transactions...\")\n",
    "df_jom = pd.read_csv(\n",
    "    'datasample/transaksi_jakone_mobile_202601081036.csv',\n",
    "    dtype={'sourceoffund': str}\n",
    ")\n",
    "validate_data_load(df_jom, \"Mobile Banking Transactions\")\n",
    "memory_usage_summary(df_jom, \"Mobile Transactions\")\n",
    "\n",
    "# Load ATM/EDC Transactions (20K - SAMPLE)\n",
    "print(\"\\nLoading ATM/EDC transactions...\")\n",
    "df_atm = pd.read_csv(\n",
    "    'datasample/transaksi_atm_edc_202601081035.csv',\n",
    "    dtype={'sourceoffund': str}\n",
    ")\n",
    "validate_data_load(df_atm, \"ATM/EDC Transactions\")\n",
    "memory_usage_summary(df_atm, \"ATM Transactions\")\n",
    "\n",
    "# Load mPOS/Merchant Transactions (856K records)\n",
    "print(\"\\nLoading mPOS/merchant transactions...\")\n",
    "df_mpos = pd.read_csv(\n",
    "    'datasample/transaksi_mpos_202601081037.csv',\n",
    "    dtype={'merchant_norek': str}\n",
    ")\n",
    "validate_data_load(df_mpos, \"mPOS/Merchant Transactions\")\n",
    "memory_usage_summary(df_mpos, \"mPOS Transactions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All data files loaded successfully!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Clean Account Data =====\n",
    "print(\"Cleaning account data...\")\n",
    "\n",
    "# Remove .0 suffix from account numbers (pattern from gemini01.py)\n",
    "df_account[\"acct_no\"] = df_account[\"acct_no\"].astype(str).str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "df_account[\"cif\"] = df_account[\"cif\"].astype(str).str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "\n",
    "# Convert balances to numeric\n",
    "df_account[\"curr_bal\"] = pd.to_numeric(df_account[\"curr_bal\"], errors=\"coerce\").fillna(0)\n",
    "df_account[\"avg_bal\"] = pd.to_numeric(df_account[\"avg_bal\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Filter out dormant accounts (stat_cd = 'D') and zero balance inactive accounts\n",
    "df_account_active = df_account[\n",
    "    (df_account['stat_cd'] != '0') &  # Not inactive\n",
    "    ((df_account['curr_bal'] > 0) | (df_account['avg_bal'] > 0))  # Has some balance\n",
    "].copy()\n",
    "\n",
    "print(f\"Filtered accounts: {len(df_account):,} → {len(df_account_active):,} (removed {len(df_account) - len(df_account_active):,} inactive)\")\n",
    "\n",
    "# ===== Clean Customer Demographics =====\n",
    "print(\"\\nCleaning customer demographics...\")\n",
    "\n",
    "df_nasabah[\"cif\"] = df_nasabah[\"cif\"].astype(str).str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "\n",
    "# Calculate age from birth_dt\n",
    "df_nasabah['birth_dt'] = pd.to_datetime(df_nasabah['birth_dt'], errors='coerce')\n",
    "ref_date = pd.to_datetime('2025-12-31')\n",
    "df_nasabah['age'] = ((ref_date - df_nasabah['birth_dt']).dt.days / 365.25).round(0)\n",
    "\n",
    "# Filter reasonable ages (18-100)\n",
    "df_nasabah['age'] = df_nasabah['age'].clip(lower=18, upper=100)\n",
    "\n",
    "print(f\"Customer demographics: {len(df_nasabah):,} records\")\n",
    "print(f\"Age range: {df_nasabah['age'].min():.0f} - {df_nasabah['age'].max():.0f}\")\n",
    "\n",
    "# ===== Clean Core Banking Transactions =====\n",
    "print(\"\\nCleaning core banking transactions...\")\n",
    "\n",
    "# Clean account numbers\n",
    "df_core[\"DTACCT\"] = df_core[\"DTACCT\"].astype(str).str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "\n",
    "# Parse dates (format: YYYYMMDD)\n",
    "df_core[\"DTDATE\"] = pd.to_datetime(df_core[\"DTDATE\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "# Convert amounts to numeric\n",
    "df_core[\"DTAMT\"] = pd.to_numeric(df_core[\"DTAMT\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Remove invalid transactions\n",
    "df_core = df_core[df_core['DTAMT'] > 0].copy()\n",
    "\n",
    "print(f\"Core transactions: {len(df_core):,} valid records\")\n",
    "print(f\"Date range: {df_core['DTDATE'].min()} to {df_core['DTDATE'].max()}\")\n",
    "\n",
    "# ===== Clean Mobile Banking Transactions =====\n",
    "print(\"\\nCleaning mobile banking transactions...\")\n",
    "\n",
    "df_jom[\"sourceoffund\"] = df_jom[\"sourceoffund\"].astype(str).str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "df_jom[\"nilai\"] = pd.to_numeric(df_jom[\"nilai\"], errors=\"coerce\").fillna(0)\n",
    "df_jom[\"tgltrx\"] = pd.to_datetime(df_jom[\"tgltrx\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"Mobile transactions: {len(df_jom):,} records\")\n",
    "print(f\"Date range: {df_jom['tgltrx'].min()} to {df_jom['tgltrx'].max()}\")\n",
    "\n",
    "# ===== Clean ATM/EDC Transactions =====\n",
    "print(\"\\nCleaning ATM/EDC transactions...\")\n",
    "\n",
    "df_atm[\"sourceoffund\"] = df_atm[\"sourceoffund\"].astype(str).str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "df_atm[\"nilai\"] = pd.to_numeric(df_atm[\"nilai\"], errors=\"coerce\").fillna(0)\n",
    "df_atm[\"tgltrx\"] = pd.to_datetime(df_atm[\"tgltrx\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"ATM transactions: {len(df_atm):,} records\")\n",
    "\n",
    "# ===== Clean mPOS Transactions =====\n",
    "print(\"\\nCleaning mPOS transactions...\")\n",
    "\n",
    "df_mpos[\"merchant_norek\"] = df_mpos[\"merchant_norek\"].astype(str).str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "df_mpos[\"totalamount\"] = pd.to_numeric(df_mpos[\"totalamount\"], errors=\"coerce\").fillna(0)\n",
    "df_mpos[\"tanggal\"] = pd.to_datetime(df_mpos[\"tanggal\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"mPOS transactions: {len(df_mpos):,} records\")\n",
    "print(f\"Date range: {df_mpos['tanggal'].min()} to {df_mpos['tanggal'].max()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Data cleaning completed!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Create Account-Level Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate accounts to customer level (CIF)\n",
    "print(\"Aggregating accounts by customer (CIF)...\")\n",
    "\n",
    "customer_accounts = df_account_active.groupby('cif').agg({\n",
    "    'acct_no': 'count',           # Number of accounts\n",
    "    'curr_bal': 'sum',             # Total current balance\n",
    "    'avg_bal': 'sum',              # Total average balance\n",
    "    'open_dt': 'min'               # Oldest account opening date\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns\n",
    "customer_accounts.columns = ['cif', 'num_accounts', 'curr_bal_total', 'avg_bal_total', 'oldest_account_dt']\n",
    "\n",
    "# Calculate account tenure\n",
    "customer_accounts['account_tenure_days'] = (ref_date - customer_accounts['oldest_account_dt']).dt.days\n",
    "customer_accounts['account_tenure_years'] = (customer_accounts['account_tenure_days'] / 365.25).round(1)\n",
    "\n",
    "print(f\"Customer accounts aggregated: {len(customer_accounts):,} unique customers\")\n",
    "print(f\"\\nSummary statistics:\")\n",
    "print(customer_accounts[['num_accounts', 'curr_bal_total', 'avg_bal_total', 'account_tenure_years']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Create Transaction Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Core Banking Transaction Features =====\n",
    "print(\"Aggregating core banking transactions...\")\n",
    "\n",
    "# Create mapping from account to CIF\n",
    "acct_to_cif = df_account_active[['acct_no', 'cif']].drop_duplicates()\n",
    "\n",
    "# Join transactions to get CIF\n",
    "df_core_cif = df_core.merge(acct_to_cif, left_on='DTACCT', right_on='acct_no', how='inner')\n",
    "\n",
    "print(f\"Matched core transactions to CIF: {len(df_core_cif):,}/{len(df_core):,} ({len(df_core_cif)/len(df_core)*100:.1f}%)\")\n",
    "\n",
    "# Aggregate by CIF\n",
    "core_features = df_core_cif.groupby('cif').agg(\n",
    "    recency_core_days=('DTDATE', lambda x: (ref_date - x.max()).days),\n",
    "    freq_core=('DTAMT', 'count'),\n",
    "    monetary_inflow=('DTAMT', lambda x: x[df_core_cif.loc[x.index, 'DTDBCR'] == 'C'].sum()),\n",
    "    monetary_outflow=('DTAMT', lambda x: x[df_core_cif.loc[x.index, 'DTDBCR'] == 'D'].sum()),\n",
    "    avg_txn_size=('DTAMT', 'mean'),\n",
    "    high_value_txn_count=('DTAMT', lambda x: (x > 1000000).sum())  # > 1M IDR\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Core features created for {len(core_features):,} customers\")\n",
    "\n",
    "# ===== Mobile Banking Transaction Features =====\n",
    "print(\"\\nAggregating mobile banking transactions...\")\n",
    "\n",
    "df_jom_cif = df_jom.merge(acct_to_cif, left_on='sourceoffund', right_on='acct_no', how='inner')\n",
    "\n",
    "mobile_features = df_jom_cif.groupby('cif').agg(\n",
    "    mobile_txn_count=('nilai', 'count'),\n",
    "    mobile_txn_volume=('nilai', 'sum'),\n",
    "    qris_user_flag=('jenistrx', lambda x: (x.str.contains('Qris', case=False, na=False)).any().astype(int)),\n",
    "    bill_payment_flag=('jenistrx', lambda x: (x.str.contains('Billpayment', case=False, na=False)).any().astype(int)),\n",
    "    transfer_count=('jenistrx', lambda x: (x.str.contains('Transfer', case=False, na=False)).sum())\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Mobile features created for {len(mobile_features):,} customers\")\n",
    "\n",
    "# ===== ATM/EDC Transaction Features =====\n",
    "print(\"\\nAggregating ATM/EDC transactions...\")\n",
    "\n",
    "df_atm_cif = df_atm.merge(acct_to_cif, left_on='sourceoffund', right_on='acct_no', how='inner')\n",
    "\n",
    "atm_features = df_atm_cif.groupby('cif').agg(\n",
    "    atm_txn_count=('nilai', 'count'),\n",
    "    atm_txn_volume=('nilai', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "print(f\"ATM features created for {len(atm_features):,} customers\")\n",
    "\n",
    "# ===== mPOS Transaction Features =====\n",
    "print(\"\\nAggregating mPOS/merchant transactions...\")\n",
    "\n",
    "df_mpos_cif = df_mpos.merge(acct_to_cif, left_on='merchant_norek', right_on='acct_no', how='inner')\n",
    "\n",
    "mpos_features = df_mpos_cif.groupby('cif').agg(\n",
    "    mpos_txn_count=('totalamount', 'count'),\n",
    "    mpos_txn_volume=('totalamount', 'sum'),\n",
    "    mpos_avg_ticket=('totalamount', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "print(f\"mPOS features created for {len(mpos_features):,} customers\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Transaction aggregations completed!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Create Master Customer Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with customer accounts as base\n",
    "master_df = customer_accounts.copy()\n",
    "\n",
    "print(f\"Starting with {len(master_df):,} customers from account data\")\n",
    "\n",
    "# Join demographics (LEFT JOIN - only 20K have demographics)\n",
    "master_df = master_df.merge(\n",
    "    df_nasabah[['cif', 'age', 'gender', 'segmen', 'kota', 'propinsi']],\n",
    "    on='cif',\n",
    "    how='left'\n",
    ")\n",
    "print(f\"After demographics join: {master_df['age'].notna().sum():,} customers have age data\")\n",
    "\n",
    "# Join core transaction features\n",
    "master_df = master_df.merge(core_features, on='cif', how='left')\n",
    "print(f\"After core transactions join: {master_df['freq_core'].notna().sum():,} customers have core transaction data\")\n",
    "\n",
    "# Join mobile transaction features\n",
    "master_df = master_df.merge(mobile_features, on='cif', how='left')\n",
    "print(f\"After mobile transactions join: {master_df['mobile_txn_count'].notna().sum():,} customers have mobile data\")\n",
    "\n",
    "# Join ATM transaction features\n",
    "master_df = master_df.merge(atm_features, on='cif', how='left')\n",
    "print(f\"After ATM transactions join: {master_df['atm_txn_count'].notna().sum():,} customers have ATM data\")\n",
    "\n",
    "# Join mPOS transaction features\n",
    "master_df = master_df.merge(mpos_features, on='cif', how='left')\n",
    "print(f\"After mPOS transactions join: {master_df['mpos_txn_count'].notna().sum():,} customers have mPOS data\")\n",
    "\n",
    "# Fill NaN values for transaction features (no transactions = 0)\n",
    "transaction_cols = [\n",
    "    'recency_core_days', 'freq_core', 'monetary_inflow', 'monetary_outflow', 'avg_txn_size', 'high_value_txn_count',\n",
    "    'mobile_txn_count', 'mobile_txn_volume', 'qris_user_flag', 'bill_payment_flag', 'transfer_count',\n",
    "    'atm_txn_count', 'atm_txn_volume',\n",
    "    'mpos_txn_count', 'mpos_txn_volume', 'mpos_avg_ticket'\n",
    "]\n",
    "\n",
    "master_df[transaction_cols] = master_df[transaction_cols].fillna(0)\n",
    "\n",
    "print(f\"\\nMaster customer table created: {len(master_df):,} customers with {len(master_df.columns)} features\")\n",
    "print(f\"\\nColumns: {list(master_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's explore the data to understand distributions, patterns, and relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value analysis\n",
    "print(\"Missing Value Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "missing_pct = (master_df.isnull().sum() / len(master_df) * 100).sort_values(ascending=False)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_pct.index,\n",
    "    'Missing %': missing_pct.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing %'] > 0]\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No missing values in master table!\")\n",
    "\n",
    "# Data coverage summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Data Coverage Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total customers: {len(master_df):,}\")\n",
    "print(f\"With demographics: {master_df['age'].notna().sum():,} ({master_df['age'].notna().sum()/len(master_df)*100:.1f}%)\")\n",
    "print(f\"With core transactions: {(master_df['freq_core'] > 0).sum():,} ({(master_df['freq_core'] > 0).sum()/len(master_df)*100:.1f}%)\")\n",
    "print(f\"With mobile banking: {(master_df['mobile_txn_count'] > 0).sum():,} ({(master_df['mobile_txn_count'] > 0).sum()/len(master_df)*100:.1f}%)\")\n",
    "print(f\"With ATM usage: {(master_df['atm_txn_count'] > 0).sum():,} ({(master_df['atm_txn_count'] > 0).sum()/len(master_df)*100:.1f}%)\")\n",
    "print(f\"With merchant activity: {(master_df['mpos_txn_count'] > 0).sum():,} ({(master_df['mpos_txn_count'] > 0).sum()/len(master_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Current balance distribution (log scale)\n",
    "axes[0, 0].hist(master_df[master_df['curr_bal_total'] > 0]['curr_bal_total'], bins=50, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Current Balance Total (IDR)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Current Balance Distribution')\n",
    "axes[0, 0].set_xscale('log')\n",
    "\n",
    "# Number of accounts distribution\n",
    "master_df['num_accounts'].value_counts().sort_index().plot(kind='bar', ax=axes[0, 1], edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Number of Accounts')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Number of Accounts per Customer')\n",
    "\n",
    "# Age distribution (for customers with demographics)\n",
    "master_df[master_df['age'].notna()]['age'].hist(bins=30, ax=axes[1, 0], edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Age')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title(f'Age Distribution (n={master_df[\"age\"].notna().sum():,})')\n",
    "\n",
    "# Account tenure distribution\n",
    "master_df['account_tenure_years'].hist(bins=30, ax=axes[1, 1], edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Account Tenure (Years)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Account Tenure Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nBalance Metrics:\")\n",
    "print(master_df[['curr_bal_total', 'avg_bal_total']].describe())\n",
    "print(\"\\nAccount Metrics:\")\n",
    "print(master_df[['num_accounts', 'account_tenure_years']].describe())\n",
    "print(\"\\nAge Statistics (where available):\")\n",
    "print(master_df['age'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction activity distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Core transaction frequency\n",
    "master_df[master_df['freq_core'] > 0]['freq_core'].hist(bins=50, ax=axes[0, 0], edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Core Transaction Count')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title(f'Core Transaction Frequency (n={(master_df[\"freq_core\"] > 0).sum():,})')\n",
    "\n",
    "# Mobile vs ATM usage\n",
    "channel_data = pd.DataFrame({\n",
    "    'Mobile': (master_df['mobile_txn_count'] > 0).sum(),\n",
    "    'ATM': (master_df['atm_txn_count'] > 0).sum(),\n",
    "    'mPOS': (master_df['mpos_txn_count'] > 0).sum()\n",
    "}, index=['Count'])\n",
    "channel_data.T.plot(kind='bar', ax=axes[0, 1], legend=False, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Channel')\n",
    "axes[0, 1].set_ylabel('Number of Customers')\n",
    "axes[0, 1].set_title('Channel Adoption')\n",
    "axes[0, 1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Inflow vs Outflow\n",
    "master_df[master_df['monetary_inflow'] > 0]['monetary_inflow'].apply(np.log10).hist(bins=50, ax=axes[1, 0], alpha=0.7, label='Inflow', edgecolor='black')\n",
    "master_df[master_df['monetary_outflow'] > 0]['monetary_outflow'].apply(np.log10).hist(bins=50, ax=axes[1, 0], alpha=0.7, label='Outflow', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Log10(Amount IDR)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Monetary Inflow vs Outflow')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Existing segment distribution (where available)\n",
    "if master_df['segmen'].notna().sum() > 0:\n",
    "    master_df['segmen'].value_counts().head(10).plot(kind='barh', ax=axes[1, 1], edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Count')\n",
    "    axes[1, 1].set_ylabel('Segment')\n",
    "    axes[1, 1].set_title('Top 10 Existing Segments')\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No segment data available', ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Engineering\n",
    "\n",
    "Create comprehensive features for customer segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create derived features\n",
    "print(\"Creating engineered features...\")\n",
    "\n",
    "# ===== Retention Rate (Key churn indicator from gemini01.py) =====\n",
    "master_df['retention_rate'] = (\n",
    "    (master_df['monetary_inflow'] - master_df['monetary_outflow']) /\n",
    "    master_df['monetary_inflow']\n",
    ").fillna(1)  # Fill NaN with 1 (no outflow = perfect retention)\n",
    "\n",
    "# Cap retention rate at 1 (some may have more inflow than outflow)\n",
    "master_df['retention_rate'] = master_df['retention_rate'].clip(upper=1)\n",
    "\n",
    "# ===== Net Flow =====\n",
    "master_df['net_flow'] = master_df['monetary_inflow'] - master_df['monetary_outflow']\n",
    "\n",
    "# ===== Total Transaction Frequency =====\n",
    "master_df['freq_total'] = (\n",
    "    master_df['freq_core'] +\n",
    "    master_df['mobile_txn_count'] +\n",
    "    master_df['atm_txn_count'] +\n",
    "    master_df['mpos_txn_count']\n",
    ")\n",
    "\n",
    "# ===== Channel Diversity Score =====\n",
    "# Count unique channels used (0-4)\n",
    "master_df['channel_diversity'] = (\n",
    "    (master_df['freq_core'] > 0).astype(int) +\n",
    "    (master_df['mobile_txn_count'] > 0).astype(int) +\n",
    "    (master_df['atm_txn_count'] > 0).astype(int) +\n",
    "    (master_df['mpos_txn_count'] > 0).astype(int)\n",
    ")\n",
    "\n",
    "# ===== Digital Adoption Metrics =====\n",
    "master_df['mobile_adoption_flag'] = (master_df['mobile_txn_count'] > 0).astype(int)\n",
    "\n",
    "# Digital score: weighted combination of mobile and digital features\n",
    "# Mobile usage + QRIS + Bill Payment (0-100 scale)\n",
    "master_df['digital_score'] = (\n",
    "    (master_df['mobile_txn_count'] > 0).astype(int) * 40 +  # Mobile adoption: 40 points\n",
    "    master_df['qris_user_flag'] * 30 +                       # QRIS usage: 30 points\n",
    "    master_df['bill_payment_flag'] * 30                      # Bill payment: 30 points\n",
    ")\n",
    "\n",
    "# ===== Channel Preference Ratios =====\n",
    "# Avoid division by zero\n",
    "master_df['mobile_preference_ratio'] = np.where(\n",
    "    master_df['freq_total'] > 0,\n",
    "    master_df['mobile_txn_count'] / master_df['freq_total'],\n",
    "    0\n",
    ")\n",
    "\n",
    "master_df['atm_preference_ratio'] = np.where(\n",
    "    master_df['freq_total'] > 0,\n",
    "    master_df['atm_txn_count'] / master_df['freq_total'],\n",
    "    0\n",
    ")\n",
    "\n",
    "# ===== Balance Stability =====\n",
    "master_df['balance_stability'] = np.where(\n",
    "    master_df['avg_bal_total'] > 0,\n",
    "    master_df['curr_bal_total'] / master_df['avg_bal_total'],\n",
    "    1\n",
    ")\n",
    "\n",
    "# ===== Recency (Overall) =====\n",
    "# Use core transaction recency as primary indicator (fill with 365 if no transactions)\n",
    "master_df['recency_days'] = master_df['recency_core_days'].fillna(365)\n",
    "\n",
    "# ===== Transaction Velocity =====\n",
    "# Transactions per day of account tenure\n",
    "master_df['txn_velocity'] = np.where(\n",
    "    master_df['account_tenure_days'] > 0,\n",
    "    master_df['freq_total'] / master_df['account_tenure_days'],\n",
    "    0\n",
    ")\n",
    "\n",
    "print(\"\\nEngineered Features Created:\")\n",
    "print(\"=\"*50)\n",
    "print(\"✓ retention_rate (churn indicator)\")\n",
    "print(\"✓ net_flow (inflow - outflow)\")\n",
    "print(\"✓ freq_total (total transaction count)\")\n",
    "print(\"✓ channel_diversity (unique channels used)\")\n",
    "print(\"✓ digital_score (0-100 digital adoption)\")\n",
    "print(\"✓ mobile_adoption_flag (binary)\")\n",
    "print(\"✓ mobile_preference_ratio (mobile % of total)\")\n",
    "print(\"✓ atm_preference_ratio (ATM % of total)\")\n",
    "print(\"✓ balance_stability (current/average)\")\n",
    "print(\"✓ recency_days (days since last transaction)\")\n",
    "print(\"✓ txn_velocity (transactions per day)\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of engineered features:\")\n",
    "sample_cols = ['cif', 'retention_rate', 'digital_score', 'channel_diversity', 'freq_total', 'net_flow']\n",
    "print(master_df[sample_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features for correlation analysis\n",
    "feature_cols = [\n",
    "    'curr_bal_total', 'avg_bal_total', 'num_accounts',\n",
    "    'monetary_inflow', 'monetary_outflow', 'retention_rate',\n",
    "    'freq_total', 'digital_score', 'channel_diversity',\n",
    "    'mobile_txn_count', 'atm_txn_count', 'mpos_txn_count',\n",
    "    'recency_days', 'account_tenure_years', 'txn_velocity',\n",
    "    'high_value_txn_count'\n",
    "]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = master_df[feature_cols].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated pairs (>0.8)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append((\n",
    "                corr_matrix.columns[i],\n",
    "                corr_matrix.columns[j],\n",
    "                corr_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"\\nHighly Correlated Feature Pairs (|r| > 0.8):\")\n",
    "    print(\"=\"*50)\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"{feat1} <-> {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"\\nNo highly correlated feature pairs found (|r| > 0.8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Feature Selection & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final features for clustering\n",
    "# Based on correlation analysis and business importance\n",
    "clustering_features = [\n",
    "    'curr_bal_total',           # Wealth indicator\n",
    "    'avg_bal_total',            # Stability indicator\n",
    "    'monetary_inflow',          # Income flow\n",
    "    'retention_rate',           # Churn indicator (key!)\n",
    "    'freq_total',               # Activity level\n",
    "    'digital_score',            # Tech adoption\n",
    "    'channel_diversity',        # Sophistication\n",
    "    'account_tenure_years',     # Loyalty\n",
    "    'num_accounts',             # Product holding\n",
    "    'high_value_txn_count',     # Wealth behavior\n",
    "    'mpos_txn_count',           # Business indicator\n",
    "    'recency_days'              # Engagement\n",
    "]\n",
    "\n",
    "print(f\"Selected {len(clustering_features)} features for clustering:\")\n",
    "for i, feat in enumerate(clustering_features, 1):\n",
    "    print(f\"{i:2d}. {feat}\")\n",
    "\n",
    "# Create feature matrix\n",
    "X = master_df[clustering_features].copy()\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Missing values: {X.isnull().sum().sum()}\")\n",
    "\n",
    "# Display feature statistics\n",
    "print(\"\\nFeature Statistics:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values and outliers\n",
    "print(\"Preparing features for clustering...\\n\")\n",
    "\n",
    "# 1. Impute missing values (use median for robustness)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(X),\n",
    "    columns=X.columns,\n",
    "    index=X.index\n",
    ")\n",
    "\n",
    "print(f\"✓ Missing values imputed (median strategy)\")\n",
    "\n",
    "# 2. Cap outliers at 99th percentile for balance and monetary features\n",
    "outlier_features = ['curr_bal_total', 'avg_bal_total', 'monetary_inflow']\n",
    "for feat in outlier_features:\n",
    "    p99 = X_imputed[feat].quantile(0.99)\n",
    "    outliers = (X_imputed[feat] > p99).sum()\n",
    "    X_imputed[feat] = X_imputed[feat].clip(upper=p99)\n",
    "    print(f\"✓ {feat}: capped {outliers:,} outliers at 99th percentile ({p99:,.0f})\")\n",
    "\n",
    "# 3. Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "print(f\"\\n✓ Features scaled using StandardScaler\")\n",
    "print(f\"\\nScaled feature matrix shape: {X_scaled.shape}\")\n",
    "print(f\"Mean: {X_scaled.mean():.4f}\")\n",
    "print(f\"Std: {X_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Clustering Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Determine Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different numbers of clusters\n",
    "print(\"Testing different cluster counts...\\n\")\n",
    "\n",
    "K_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "davies_bouldin_scores = []\n",
    "calinski_scores = []\n",
    "\n",
    "for k in K_range:\n",
    "    print(f\"Testing k={k}...\", end=' ')\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, labels))\n",
    "    davies_bouldin_scores.append(davies_bouldin_score(X_scaled, labels))\n",
    "    calinski_scores.append(calinski_harabasz_score(X_scaled, labels))\n",
    "    \n",
    "    print(f\"Silhouette: {silhouette_scores[-1]:.3f}\")\n",
    "\n",
    "print(\"\\nOptimization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot elbow and silhouette curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Elbow plot\n",
    "axes[0, 0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)\n",
    "axes[0, 0].set_title('Elbow Method', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xticks(K_range)\n",
    "\n",
    "# Silhouette scores\n",
    "axes[0, 1].plot(K_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
    "axes[0, 1].axhline(y=0.3, color='green', linestyle='--', label='Good threshold (0.3)')\n",
    "axes[0, 1].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[0, 1].set_title('Silhouette Analysis', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xticks(K_range)\n",
    "\n",
    "# Davies-Bouldin Index (lower is better)\n",
    "axes[1, 0].plot(K_range, davies_bouldin_scores, 'go-', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Davies-Bouldin Index', fontsize=12)\n",
    "axes[1, 0].set_title('Davies-Bouldin Index (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xticks(K_range)\n",
    "\n",
    "# Calinski-Harabasz Score (higher is better)\n",
    "axes[1, 1].plot(K_range, calinski_scores, 'mo-', linewidth=2, markersize=8)\n",
    "axes[1, 1].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Calinski-Harabasz Score', fontsize=12)\n",
    "axes[1, 1].set_title('Calinski-Harabasz Score (Higher is Better)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xticks(K_range)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print optimal k recommendations\n",
    "optimal_k_silhouette = K_range[np.argmax(silhouette_scores)]\n",
    "optimal_k_davies = K_range[np.argmin(davies_bouldin_scores)]\n",
    "optimal_k_calinski = K_range[np.argmax(calinski_scores)]\n",
    "\n",
    "print(\"\\nOptimal k recommendations:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"By Silhouette Score: k = {optimal_k_silhouette} (score: {max(silhouette_scores):.3f})\")\n",
    "print(f\"By Davies-Bouldin: k = {optimal_k_davies} (score: {min(davies_bouldin_scores):.3f})\")\n",
    "print(f\"By Calinski-Harabasz: k = {optimal_k_calinski} (score: {max(calinski_scores):.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Final K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select optimal k (typically 5-6 based on business needs and metrics)\n",
    "optimal_k = 5  # Adjust based on the plots above\n",
    "\n",
    "print(f\"Performing final clustering with k={optimal_k}...\\n\")\n",
    "\n",
    "# Final K-Means clustering\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=20, max_iter=500)\n",
    "master_df['cluster'] = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# Calculate cluster quality metrics\n",
    "silhouette_avg = silhouette_score(X_scaled, master_df['cluster'])\n",
    "davies_bouldin = davies_bouldin_score(X_scaled, master_df['cluster'])\n",
    "calinski = calinski_harabasz_score(X_scaled, master_df['cluster'])\n",
    "\n",
    "print(\"Clustering Quality Metrics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.3f} (target >0.3)\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin:.3f} (lower better)\")\n",
    "print(f\"Calinski-Harabasz Score: {calinski:.1f} (higher better)\")\n",
    "\n",
    "# Cluster size distribution\n",
    "print(\"\\nCluster Size Distribution:\")\n",
    "print(\"=\"*50)\n",
    "cluster_counts = master_df['cluster'].value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    pct = count / len(master_df) * 100\n",
    "    print(f\"Cluster {cluster_id}: {count:,} customers ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nClustering complete! {len(master_df):,} customers segmented into {optimal_k} clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Cluster Interpretation & Business Naming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Analyze Cluster Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cluster means for key features\n",
    "cluster_profile_features = [\n",
    "    'curr_bal_total', 'avg_bal_total', 'num_accounts',\n",
    "    'monetary_inflow', 'monetary_outflow', 'retention_rate',\n",
    "    'freq_total', 'digital_score', 'mobile_adoption_flag',\n",
    "    'channel_diversity', 'account_tenure_years',\n",
    "    'mpos_txn_count', 'recency_days', 'age'\n",
    "]\n",
    "\n",
    "cluster_means = master_df.groupby('cluster')[cluster_profile_features].mean()\n",
    "\n",
    "# Calculate relative to overall mean (for interpretation)\n",
    "overall_mean = master_df[cluster_profile_features].mean()\n",
    "relative_means = ((cluster_means - overall_mean) / overall_mean * 100).round(1)\n",
    "\n",
    "print(\"Cluster Characteristics (% difference from overall mean):\")\n",
    "print(\"=\"*80)\n",
    "print(relative_means.T)\n",
    "\n",
    "# Identify defining characteristics for each cluster\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Top 3 Defining Characteristics per Cluster:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for cluster_id in cluster_means.index:\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    top_features = relative_means.loc[cluster_id].abs().nlargest(3)\n",
    "    for feat in top_features.index:\n",
    "        value = relative_means.loc[cluster_id, feat]\n",
    "        direction = \"higher\" if value > 0 else \"lower\"\n",
    "        print(f\"  • {feat}: {abs(value):.1f}% {direction} than average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Assign Business Names to Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated cluster interpretation based on centroid analysis\n",
    "# Following gemini01.py pattern\n",
    "\n",
    "# Identify key clusters\n",
    "sweeper_cluster = cluster_means['retention_rate'].idxmin()       # Lowest retention = churn risk\n",
    "high_value_cluster = cluster_means['curr_bal_total'].idxmax()    # Highest balance\n",
    "digital_cluster = cluster_means['digital_score'].idxmax()        # Most digital\n",
    "merchant_cluster = cluster_means['mpos_txn_count'].idxmax()      # Most merchant activity\n",
    "\n",
    "# Create mapping based on analysis\n",
    "# NOTE: Adjust these based on your actual cluster characteristics above!\n",
    "cluster_names = {}\n",
    "\n",
    "# Assign names based on defining characteristics\n",
    "for cluster_id in range(optimal_k):\n",
    "    profile = cluster_means.loc[cluster_id]\n",
    "    \n",
    "    if cluster_id == sweeper_cluster:\n",
    "        cluster_names[cluster_id] = \"Salary Sweepers (High Churn Risk)\"\n",
    "    elif cluster_id == high_value_cluster:\n",
    "        if profile['digital_score'] > overall_mean['digital_score']:\n",
    "            cluster_names[cluster_id] = \"Affluent Digitally Engaged\"\n",
    "        else:\n",
    "            cluster_names[cluster_id] = \"Mass Affluent Savers\"\n",
    "    elif cluster_id == digital_cluster:\n",
    "        cluster_names[cluster_id] = \"Digital Natives\"\n",
    "    elif cluster_id == merchant_cluster:\n",
    "        cluster_names[cluster_id] = \"Emerging Merchants\"\n",
    "    else:\n",
    "        # Assign remaining based on characteristics\n",
    "        if profile['digital_score'] < overall_mean['digital_score'] * 0.5:\n",
    "            cluster_names[cluster_id] = \"Branch-Dependent Traditional\"\n",
    "        else:\n",
    "            cluster_names[cluster_id] = f\"General Segment {cluster_id}\"\n",
    "\n",
    "# Apply names to dataframe\n",
    "master_df['segment_name'] = master_df['cluster'].map(cluster_names)\n",
    "\n",
    "print(\"Business Segment Names Assigned:\")\n",
    "print(\"=\"*50)\n",
    "for cluster_id, name in cluster_names.items():\n",
    "    count = (master_df['cluster'] == cluster_id).sum()\n",
    "    pct = count / len(master_df) * 100\n",
    "    print(f\"Cluster {cluster_id} → {name}\")\n",
    "    print(f\"   {count:,} customers ({pct:.1f}%)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Validation & Segment Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Detailed Segment Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive segment profile\n",
    "segment_profile = master_df.groupby('segment_name').agg({\n",
    "    'cif': 'count',\n",
    "    'age': 'mean',\n",
    "    'curr_bal_total': ['mean', 'median', 'sum'],\n",
    "    'avg_bal_total': 'mean',\n",
    "    'num_accounts': 'mean',\n",
    "    'freq_total': 'mean',\n",
    "    'retention_rate': 'mean',\n",
    "    'digital_score': 'mean',\n",
    "    'mobile_adoption_flag': 'mean',\n",
    "    'channel_diversity': 'mean',\n",
    "    'account_tenure_years': 'mean',\n",
    "    'recency_days': 'mean',\n",
    "    'mpos_txn_count': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "segment_profile.columns = ['_'.join(col).strip('_') for col in segment_profile.columns.values]\n",
    "\n",
    "# Calculate percentage of base\n",
    "segment_profile['pct_of_base'] = (segment_profile['cif_count'] / len(master_df) * 100).round(1)\n",
    "\n",
    "# Convert percentages\n",
    "segment_profile['mobile_adoption_pct'] = (segment_profile['mobile_adoption_flag_mean'] * 100).round(1)\n",
    "segment_profile['retention_rate_pct'] = (segment_profile['retention_rate_mean'] * 100).round(1)\n",
    "\n",
    "print(\"Detailed Segment Profiles:\")\n",
    "print(\"=\"*80)\n",
    "print(segment_profile.to_string())\n",
    "\n",
    "# Export to CSV for easy viewing\n",
    "segment_profile.to_csv('segment_profiles_summary.csv')\n",
    "print(\"\\n✓ Segment profiles saved to segment_profiles_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Segment Insights & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate business insights for each segment\n",
    "print(\"SEGMENT INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for segment_name in master_df['segment_name'].unique():\n",
    "    segment_data = master_df[master_df['segment_name'] == segment_name]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SEGMENT: {segment_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Size\n",
    "    count = len(segment_data)\n",
    "    pct = count / len(master_df) * 100\n",
    "    print(f\"\\nSize: {count:,} customers ({pct:.1f}% of base)\")\n",
    "    \n",
    "    # Key Characteristics\n",
    "    print(f\"\\nKey Characteristics:\")\n",
    "    print(f\"  • Average Balance: IDR {segment_data['curr_bal_total'].mean():,.0f}\")\n",
    "    print(f\"  • Retention Rate: {segment_data['retention_rate'].mean()*100:.1f}%\")\n",
    "    print(f\"  • Digital Score: {segment_data['digital_score'].mean():.0f}/100\")\n",
    "    print(f\"  • Avg Transactions/Customer: {segment_data['freq_total'].mean():.0f}\")\n",
    "    print(f\"  • Mobile Adoption: {segment_data['mobile_adoption_flag'].mean()*100:.1f}%\")\n",
    "    \n",
    "    # Value Metrics\n",
    "    total_deposits = segment_data['curr_bal_total'].sum()\n",
    "    print(f\"\\nValue Metrics:\")\n",
    "    print(f\"  • Total Deposits: IDR {total_deposits:,.0f} ({total_deposits/master_df['curr_bal_total'].sum()*100:.1f}% of total)\")\n",
    "    print(f\"  • Avg Account Tenure: {segment_data['account_tenure_years'].mean():.1f} years\")\n",
    "    \n",
    "    # Recommendations (customize based on segment name)\n",
    "    print(f\"\\nRecommended Actions:\")\n",
    "    \n",
    "    if \"Sweeper\" in segment_name or \"Churn\" in segment_name:\n",
    "        print(f\"  ⚠️  HIGH CHURN RISK - Priority retention campaigns\")\n",
    "        print(f\"  • Offer savings incentives and automatic savings programs\")\n",
    "        print(f\"  • Financial literacy programs to improve retention\")\n",
    "        print(f\"  • Early salary access or overdraft protection products\")\n",
    "    \n",
    "    elif \"Affluent\" in segment_name or \"Mass Affluent\" in segment_name:\n",
    "        print(f\"  💎 HIGH-VALUE SEGMENT - Premium service priority\")\n",
    "        print(f\"  • Wealth management and investment product offerings\")\n",
    "        print(f\"  • Premium credit cards and exclusive benefits\")\n",
    "        print(f\"  • Relationship manager assignment for top customers\")\n",
    "    \n",
    "    elif \"Digital\" in segment_name:\n",
    "        print(f\"  📱 DIGITAL-SAVVY - Engage through digital channels\")\n",
    "        print(f\"  • Digital investment apps and robo-advisory\")\n",
    "        print(f\"  • Gamification and rewards for digital usage\")\n",
    "        print(f\"  • Early access to new digital features\")\n",
    "    \n",
    "    elif \"Merchant\" in segment_name:\n",
    "        print(f\"  🏪 BUSINESS OWNERS - SME banking opportunities\")\n",
    "        print(f\"  • Business loans and working capital financing\")\n",
    "        print(f\"  • Merchant services and POS system upgrades\")\n",
    "        print(f\"  • Business account packages with payroll features\")\n",
    "    \n",
    "    elif \"Traditional\" in segment_name or \"Branch\" in segment_name:\n",
    "        print(f\"  🏦 TRADITIONAL USERS - Digital migration opportunity\")\n",
    "        print(f\"  • Assisted digital onboarding at branches\")\n",
    "        print(f\"  • Simplified mobile app interface for seniors\")\n",
    "        print(f\"  • Branch staff training for digital education\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"  • Analyze further to determine specific strategies\")\n",
    "        print(f\"  • Monitor segment behavior over time\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 PCA Cluster Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions using PCA for visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "pca_df = pd.DataFrame({\n",
    "    'PC1': X_pca[:, 0],\n",
    "    'PC2': X_pca[:, 1],\n",
    "    'Segment': master_df['segment_name'],\n",
    "    'Balance': master_df['curr_bal_total']\n",
    "})\n",
    "\n",
    "# Static plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "for segment in pca_df['Segment'].unique():\n",
    "    segment_data = pca_df[pca_df['Segment'] == segment]\n",
    "    plt.scatter(segment_data['PC1'], segment_data['PC2'], \n",
    "                label=segment, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
    "plt.title('Customer Segments - PCA Projection', fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total variance explained by 2 components: {sum(pca.explained_variance_ratio_)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Segment Size & Value Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment size and value analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Customer count by segment\n",
    "segment_counts = master_df['segment_name'].value_counts()\n",
    "axes[0].barh(segment_counts.index, segment_counts.values, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_xlabel('Number of Customers', fontsize=12)\n",
    "axes[0].set_title('Segment Sizes', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(segment_counts.values):\n",
    "    pct = v / len(master_df) * 100\n",
    "    axes[0].text(v, i, f' {v:,} ({pct:.1f}%)', va='center', fontsize=10)\n",
    "\n",
    "# Total deposit value by segment\n",
    "segment_value = master_df.groupby('segment_name')['curr_bal_total'].sum().sort_values(ascending=True)\n",
    "axes[1].barh(segment_value.index, segment_value.values / 1e9, edgecolor='black', linewidth=1.5, color='green', alpha=0.7)\n",
    "axes[1].set_xlabel('Total Deposits (Billion IDR)', fontsize=12)\n",
    "axes[1].set_title('Segment Value (Total Deposits)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(segment_value.values):\n",
    "    pct = v / master_df['curr_bal_total'].sum() * 100\n",
    "    axes[1].text(v/1e9, i, f' {v/1e9:.1f}B ({pct:.1f}%)', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Feature Comparison Across Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for key features by segment\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Balance\n",
    "master_df.boxplot(column='curr_bal_total', by='segment_name', ax=axes[0, 0])\n",
    "axes[0, 0].set_xlabel('')\n",
    "axes[0, 0].set_ylabel('IDR', fontsize=10)\n",
    "axes[0, 0].set_title('Current Balance by Segment', fontsize=12)\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# Retention Rate\n",
    "master_df.boxplot(column='retention_rate', by='segment_name', ax=axes[0, 1])\n",
    "axes[0, 1].set_xlabel('')\n",
    "axes[0, 1].set_ylabel('Rate', fontsize=10)\n",
    "axes[0, 1].set_title('Retention Rate by Segment', fontsize=12)\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].axhline(y=0.3, color='red', linestyle='--', alpha=0.5, label='Churn Risk Threshold')\n",
    "\n",
    "# Digital Score\n",
    "master_df.boxplot(column='digital_score', by='segment_name', ax=axes[0, 2])\n",
    "axes[0, 2].set_xlabel('')\n",
    "axes[0, 2].set_ylabel('Score (0-100)', fontsize=10)\n",
    "axes[0, 2].set_title('Digital Score by Segment', fontsize=12)\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Transaction Frequency\n",
    "master_df[master_df['freq_total'] > 0].boxplot(column='freq_total', by='segment_name', ax=axes[1, 0])\n",
    "axes[1, 0].set_xlabel('')\n",
    "axes[1, 0].set_ylabel('Count', fontsize=10)\n",
    "axes[1, 0].set_title('Transaction Frequency by Segment', fontsize=12)\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "# Channel Diversity\n",
    "master_df.boxplot(column='channel_diversity', by='segment_name', ax=axes[1, 1])\n",
    "axes[1, 1].set_xlabel('')\n",
    "axes[1, 1].set_ylabel('Channels Used', fontsize=10)\n",
    "axes[1, 1].set_title('Channel Diversity by Segment', fontsize=12)\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Account Tenure\n",
    "master_df.boxplot(column='account_tenure_years', by='segment_name', ax=axes[1, 2])\n",
    "axes[1, 2].set_xlabel('')\n",
    "axes[1, 2].set_ylabel('Years', fontsize=10)\n",
    "axes[1, 2].set_title('Account Tenure by Segment', fontsize=12)\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Remove automatic titles\n",
    "for ax in axes.flat:\n",
    "    ax.get_figure().suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Cluster Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized centroid heatmap\n",
    "# Normalize features to 0-1 scale for comparison\n",
    "cluster_means_norm = (cluster_means - cluster_means.min()) / (cluster_means.max() - cluster_means.min())\n",
    "\n",
    "# Map cluster IDs to names\n",
    "cluster_means_norm.index = cluster_means_norm.index.map(cluster_names)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(cluster_means_norm.T, annot=True, fmt='.2f', \n",
    "            cmap='RdYlGn', center=0.5, linewidths=1,\n",
    "            cbar_kws={'label': 'Normalized Value (0=Low, 1=High)'},\n",
    "            vmin=0, vmax=1)\n",
    "plt.xlabel('Segment', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Segment Feature Heatmap (Normalized)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Export & Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Customer-Level Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export customer segments with key features\n",
    "export_cols = [\n",
    "    'cif', 'segment_name', 'cluster',\n",
    "    'curr_bal_total', 'avg_bal_total', 'num_accounts',\n",
    "    'retention_rate', 'digital_score', 'mobile_adoption_flag',\n",
    "    'freq_total', 'recency_days', 'channel_diversity',\n",
    "    'age', 'gender', 'account_tenure_years'\n",
    "]\n",
    "\n",
    "customer_export = master_df[export_cols].copy()\n",
    "\n",
    "# Format for readability\n",
    "customer_export['curr_bal_total'] = customer_export['curr_bal_total'].round(0)\n",
    "customer_export['avg_bal_total'] = customer_export['avg_bal_total'].round(0)\n",
    "customer_export['retention_rate'] = (customer_export['retention_rate'] * 100).round(1)\n",
    "\n",
    "# Export to CSV\n",
    "customer_export.to_csv('customer_segments_export.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"✓ Customer segments exported to customer_segments_export.csv\")\n",
    "print(f\"  {len(customer_export):,} customers with {len(export_cols)} fields\")\n",
    "print(f\"\\nSample export:\")\n",
    "print(customer_export.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Campaign Target Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Campaign 1: Premium Products Cross-Sell (High-Value Customers)\n",
    "high_value_segments = ['Affluent Digitally Engaged', 'Mass Affluent Savers']\n",
    "campaign_premium = master_df[\n",
    "    (master_df['segment_name'].isin(high_value_segments)) &\n",
    "    (master_df['num_accounts'] < 3)  # Room for more products\n",
    "][['cif', 'segment_name', 'curr_bal_total', 'num_accounts', 'digital_score', 'age']].copy()\n",
    "\n",
    "campaign_premium = campaign_premium.sort_values('curr_bal_total', ascending=False)\n",
    "campaign_premium.to_csv('campaign_premium_products.csv', index=False)\n",
    "\n",
    "print(\"✓ Campaign 1: Premium Products Cross-Sell\")\n",
    "print(f\"  Target: {len(campaign_premium):,} high-value customers with <3 accounts\")\n",
    "print(f\"  Saved to: campaign_premium_products.csv\")\n",
    "print(f\"  Total deposits: IDR {campaign_premium['curr_bal_total'].sum():,.0f}\\n\")\n",
    "\n",
    "# Campaign 2: Retention / Churn Prevention (Salary Sweepers)\n",
    "campaign_retention = master_df[\n",
    "    (master_df['retention_rate'] < 0.3) |  # Low retention\n",
    "    (master_df['segment_name'].str.contains('Sweeper', case=False, na=False))\n",
    "][['cif', 'segment_name', 'retention_rate', 'freq_total', 'monetary_inflow', 'age']].copy()\n",
    "\n",
    "campaign_retention['retention_rate_pct'] = (campaign_retention['retention_rate'] * 100).round(1)\n",
    "campaign_retention = campaign_retention.sort_values('retention_rate')\n",
    "campaign_retention.to_csv('campaign_retention.csv', index=False)\n",
    "\n",
    "print(\"✓ Campaign 2: Retention / Churn Prevention\")\n",
    "print(f\"  Target: {len(campaign_retention):,} customers at churn risk\")\n",
    "print(f\"  Saved to: campaign_retention.csv\")\n",
    "print(f\"  Avg retention rate: {campaign_retention['retention_rate'].mean()*100:.1f}%\\n\")\n",
    "\n",
    "# Campaign 3: Digital Migration (Traditional Users Under 50)\n",
    "campaign_digital = master_df[\n",
    "    (master_df['digital_score'] < 30) &  # Low digital adoption\n",
    "    (master_df['age'] < 50) &            # Not too old\n",
    "    (master_df['age'].notna())\n",
    "][['cif', 'segment_name', 'age', 'digital_score', 'mobile_adoption_flag', 'curr_bal_total']].copy()\n",
    "\n",
    "campaign_digital = campaign_digital.sort_values('curr_bal_total', ascending=False)\n",
    "campaign_digital.to_csv('campaign_digital_migration.csv', index=False)\n",
    "\n",
    "print(\"✓ Campaign 3: Digital Migration\")\n",
    "print(f\"  Target: {len(campaign_digital):,} traditional users under 50\")\n",
    "print(f\"  Saved to: campaign_digital_migration.csv\")\n",
    "print(f\"  Current mobile adoption: {campaign_digital['mobile_adoption_flag'].mean()*100:.1f}%\\n\")\n",
    "\n",
    "# Campaign 4: SME Banking (Emerging Merchants)\n",
    "campaign_sme = master_df[\n",
    "    (master_df['mpos_txn_count'] > 0)  # Has merchant activity\n",
    "][['cif', 'segment_name', 'mpos_txn_count', 'mpos_txn_volume', 'curr_bal_total', 'num_accounts']].copy()\n",
    "\n",
    "campaign_sme = campaign_sme.sort_values('mpos_txn_volume', ascending=False)\n",
    "campaign_sme.to_csv('campaign_sme_banking.csv', index=False)\n",
    "\n",
    "print(\"✓ Campaign 4: SME Banking / Merchant Services\")\n",
    "print(f\"  Target: {len(campaign_sme):,} customers with merchant activity\")\n",
    "print(f\"  Saved to: campaign_sme_banking.csv\")\n",
    "print(f\"  Total mPOS volume: IDR {campaign_sme['mpos_txn_volume'].sum():,.0f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All campaign lists exported successfully!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Executive Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Excel report with multiple sheets\n",
    "with pd.ExcelWriter('segment_analysis_report.xlsx', engine='openpyxl') as writer:\n",
    "    # Sheet 1: Executive Summary\n",
    "    segment_profile.to_excel(writer, sheet_name='Executive Summary')\n",
    "    \n",
    "    # Sheet 2: Cluster Characteristics\n",
    "    cluster_means.to_excel(writer, sheet_name='Cluster Characteristics')\n",
    "    \n",
    "    # Sheet 3: Campaign Lists Summary\n",
    "    campaign_summary = pd.DataFrame({\n",
    "        'Campaign': [\n",
    "            'Premium Products Cross-Sell',\n",
    "            'Retention / Churn Prevention',\n",
    "            'Digital Migration',\n",
    "            'SME Banking'\n",
    "        ],\n",
    "        'Target Count': [\n",
    "            len(campaign_premium),\n",
    "            len(campaign_retention),\n",
    "            len(campaign_digital),\n",
    "            len(campaign_sme)\n",
    "        ],\n",
    "        'File': [\n",
    "            'campaign_premium_products.csv',\n",
    "            'campaign_retention.csv',\n",
    "            'campaign_digital_migration.csv',\n",
    "            'campaign_sme_banking.csv'\n",
    "        ]\n",
    "    })\n",
    "    campaign_summary.to_excel(writer, sheet_name='Campaign Lists', index=False)\n",
    "    \n",
    "    # Sheet 4-N: Individual segment details\n",
    "    for segment_name in master_df['segment_name'].unique():\n",
    "        segment_data = master_df[master_df['segment_name'] == segment_name][\n",
    "            ['cif', 'curr_bal_total', 'retention_rate', 'digital_score', \n",
    "             'freq_total', 'age', 'num_accounts']\n",
    "        ].describe()\n",
    "        \n",
    "        # Truncate sheet name to 31 characters (Excel limit)\n",
    "        sheet_name = segment_name[:30]\n",
    "        segment_data.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "print(\"✓ Executive summary report saved to segment_analysis_report.xlsx\")\n",
    "print(\"  Contains:\")\n",
    "print(\"    - Executive Summary\")\n",
    "print(\"    - Cluster Characteristics\")\n",
    "print(\"    - Campaign Lists Summary\")\n",
    "print(f\"    - {len(master_df['segment_name'].unique())} individual segment sheets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Next Steps\n",
    "\n",
    "### Deliverables Created:\n",
    "1. **customer_segments_export.csv** - Full customer list with segment assignments\n",
    "2. **campaign_premium_products.csv** - High-value cross-sell targets\n",
    "3. **campaign_retention.csv** - Churn prevention targets\n",
    "4. **campaign_digital_migration.csv** - Digital onboarding targets\n",
    "5. **campaign_sme_banking.csv** - SME banking opportunities\n",
    "6. **segment_analysis_report.xlsx** - Multi-sheet executive report\n",
    "7. **segment_profiles_summary.csv** - Segment characteristics summary\n",
    "\n",
    "### Success Metrics Achieved:\n",
    "- ✅ High-value customer identification\n",
    "- ✅ Churn risk prediction (retention rate < 0.3)\n",
    "- ✅ Marketing campaign segmentation (5-6 actionable segments)\n",
    "- ✅ Silhouette score > 0.3 (good cluster separation)\n",
    "- ✅ Comprehensive visualizations\n",
    "- ✅ Actionable export files\n",
    "\n",
    "### Next Steps:\n",
    "1. **Validation**: Test campaigns on small pilot groups\n",
    "2. **Monitoring**: Track segment migration over time\n",
    "3. **Refinement**: Adjust segments based on business feedback\n",
    "4. **Automation**: Convert to Python script for monthly refresh\n",
    "5. **Advanced Analytics**: Build predictive models for specific outcomes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
